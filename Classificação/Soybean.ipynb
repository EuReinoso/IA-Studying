{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Soybean.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gLS0JIgth-o"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSTfQGEltxNE"
      },
      "source": [
        "dataset = pd.read_csv('soybean.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "Af4AvfYnt4_9",
        "outputId": "16d8340c-d3d9-4dd6-8257-16ee4e51af03"
      },
      "source": [
        "dataset.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>plant-stand</th>\n",
              "      <th>precip</th>\n",
              "      <th>temp</th>\n",
              "      <th>hail</th>\n",
              "      <th>crop-hist</th>\n",
              "      <th>area-damaged</th>\n",
              "      <th>severity</th>\n",
              "      <th>seed-tmt</th>\n",
              "      <th>germination</th>\n",
              "      <th>plant-growth</th>\n",
              "      <th>leaves</th>\n",
              "      <th>leafspots-halo</th>\n",
              "      <th>leafspots-marg</th>\n",
              "      <th>leafspot-size</th>\n",
              "      <th>leaf-shread</th>\n",
              "      <th>leaf-malf</th>\n",
              "      <th>leaf-mild</th>\n",
              "      <th>stem</th>\n",
              "      <th>lodging</th>\n",
              "      <th>stem-cankers</th>\n",
              "      <th>canker-lesion</th>\n",
              "      <th>fruiting-bodies</th>\n",
              "      <th>external-decay</th>\n",
              "      <th>mycelium</th>\n",
              "      <th>int-discolor</th>\n",
              "      <th>sclerotia</th>\n",
              "      <th>fruit-pods</th>\n",
              "      <th>fruit-spots</th>\n",
              "      <th>seed</th>\n",
              "      <th>mold-growth</th>\n",
              "      <th>seed-discolor</th>\n",
              "      <th>seed-size</th>\n",
              "      <th>shriveling</th>\n",
              "      <th>roots</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>october</td>\n",
              "      <td>normal</td>\n",
              "      <td>gt-norm</td>\n",
              "      <td>norm</td>\n",
              "      <td>yes</td>\n",
              "      <td>same-lst-yr</td>\n",
              "      <td>low-areas</td>\n",
              "      <td>pot-severe</td>\n",
              "      <td>none</td>\n",
              "      <td>90-100</td>\n",
              "      <td>abnorm</td>\n",
              "      <td>abnorm</td>\n",
              "      <td>absent</td>\n",
              "      <td>dna</td>\n",
              "      <td>dna</td>\n",
              "      <td>absent</td>\n",
              "      <td>absent</td>\n",
              "      <td>absent</td>\n",
              "      <td>abnorm</td>\n",
              "      <td>no</td>\n",
              "      <td>above-sec-nde</td>\n",
              "      <td>brown</td>\n",
              "      <td>present</td>\n",
              "      <td>firm-and-dry</td>\n",
              "      <td>absent</td>\n",
              "      <td>none</td>\n",
              "      <td>absent</td>\n",
              "      <td>norm</td>\n",
              "      <td>dna</td>\n",
              "      <td>norm</td>\n",
              "      <td>absent</td>\n",
              "      <td>absent</td>\n",
              "      <td>norm</td>\n",
              "      <td>absent</td>\n",
              "      <td>norm</td>\n",
              "      <td>diaporthe-stem-canker</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>august</td>\n",
              "      <td>normal</td>\n",
              "      <td>gt-norm</td>\n",
              "      <td>norm</td>\n",
              "      <td>yes</td>\n",
              "      <td>same-lst-two-yrs</td>\n",
              "      <td>scattered</td>\n",
              "      <td>severe</td>\n",
              "      <td>fungicide</td>\n",
              "      <td>80-89</td>\n",
              "      <td>abnorm</td>\n",
              "      <td>abnorm</td>\n",
              "      <td>absent</td>\n",
              "      <td>dna</td>\n",
              "      <td>dna</td>\n",
              "      <td>absent</td>\n",
              "      <td>absent</td>\n",
              "      <td>absent</td>\n",
              "      <td>abnorm</td>\n",
              "      <td>yes</td>\n",
              "      <td>above-sec-nde</td>\n",
              "      <td>brown</td>\n",
              "      <td>present</td>\n",
              "      <td>firm-and-dry</td>\n",
              "      <td>absent</td>\n",
              "      <td>none</td>\n",
              "      <td>absent</td>\n",
              "      <td>norm</td>\n",
              "      <td>dna</td>\n",
              "      <td>norm</td>\n",
              "      <td>absent</td>\n",
              "      <td>absent</td>\n",
              "      <td>norm</td>\n",
              "      <td>absent</td>\n",
              "      <td>norm</td>\n",
              "      <td>diaporthe-stem-canker</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>july</td>\n",
              "      <td>normal</td>\n",
              "      <td>gt-norm</td>\n",
              "      <td>norm</td>\n",
              "      <td>yes</td>\n",
              "      <td>same-lst-yr</td>\n",
              "      <td>scattered</td>\n",
              "      <td>severe</td>\n",
              "      <td>fungicide</td>\n",
              "      <td>lt-80</td>\n",
              "      <td>abnorm</td>\n",
              "      <td>abnorm</td>\n",
              "      <td>absent</td>\n",
              "      <td>dna</td>\n",
              "      <td>dna</td>\n",
              "      <td>absent</td>\n",
              "      <td>absent</td>\n",
              "      <td>absent</td>\n",
              "      <td>abnorm</td>\n",
              "      <td>yes</td>\n",
              "      <td>above-sec-nde</td>\n",
              "      <td>dna</td>\n",
              "      <td>present</td>\n",
              "      <td>firm-and-dry</td>\n",
              "      <td>absent</td>\n",
              "      <td>none</td>\n",
              "      <td>absent</td>\n",
              "      <td>norm</td>\n",
              "      <td>dna</td>\n",
              "      <td>norm</td>\n",
              "      <td>absent</td>\n",
              "      <td>absent</td>\n",
              "      <td>norm</td>\n",
              "      <td>absent</td>\n",
              "      <td>norm</td>\n",
              "      <td>diaporthe-stem-canker</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>july</td>\n",
              "      <td>normal</td>\n",
              "      <td>gt-norm</td>\n",
              "      <td>norm</td>\n",
              "      <td>yes</td>\n",
              "      <td>same-lst-yr</td>\n",
              "      <td>scattered</td>\n",
              "      <td>severe</td>\n",
              "      <td>none</td>\n",
              "      <td>80-89</td>\n",
              "      <td>abnorm</td>\n",
              "      <td>abnorm</td>\n",
              "      <td>absent</td>\n",
              "      <td>dna</td>\n",
              "      <td>dna</td>\n",
              "      <td>absent</td>\n",
              "      <td>absent</td>\n",
              "      <td>absent</td>\n",
              "      <td>abnorm</td>\n",
              "      <td>yes</td>\n",
              "      <td>above-sec-nde</td>\n",
              "      <td>dna</td>\n",
              "      <td>present</td>\n",
              "      <td>firm-and-dry</td>\n",
              "      <td>absent</td>\n",
              "      <td>none</td>\n",
              "      <td>absent</td>\n",
              "      <td>norm</td>\n",
              "      <td>dna</td>\n",
              "      <td>norm</td>\n",
              "      <td>absent</td>\n",
              "      <td>absent</td>\n",
              "      <td>norm</td>\n",
              "      <td>absent</td>\n",
              "      <td>norm</td>\n",
              "      <td>diaporthe-stem-canker</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>october</td>\n",
              "      <td>normal</td>\n",
              "      <td>gt-norm</td>\n",
              "      <td>norm</td>\n",
              "      <td>yes</td>\n",
              "      <td>same-lst-two-yrs</td>\n",
              "      <td>scattered</td>\n",
              "      <td>pot-severe</td>\n",
              "      <td>none</td>\n",
              "      <td>lt-80</td>\n",
              "      <td>abnorm</td>\n",
              "      <td>abnorm</td>\n",
              "      <td>absent</td>\n",
              "      <td>dna</td>\n",
              "      <td>dna</td>\n",
              "      <td>absent</td>\n",
              "      <td>absent</td>\n",
              "      <td>absent</td>\n",
              "      <td>abnorm</td>\n",
              "      <td>yes</td>\n",
              "      <td>above-sec-nde</td>\n",
              "      <td>brown</td>\n",
              "      <td>present</td>\n",
              "      <td>firm-and-dry</td>\n",
              "      <td>absent</td>\n",
              "      <td>none</td>\n",
              "      <td>absent</td>\n",
              "      <td>norm</td>\n",
              "      <td>dna</td>\n",
              "      <td>norm</td>\n",
              "      <td>absent</td>\n",
              "      <td>absent</td>\n",
              "      <td>norm</td>\n",
              "      <td>absent</td>\n",
              "      <td>norm</td>\n",
              "      <td>diaporthe-stem-canker</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      date plant-stand   precip  ... shriveling roots                  class\n",
              "0  october      normal  gt-norm  ...     absent  norm  diaporthe-stem-canker\n",
              "1   august      normal  gt-norm  ...     absent  norm  diaporthe-stem-canker\n",
              "2     july      normal  gt-norm  ...     absent  norm  diaporthe-stem-canker\n",
              "3     july      normal  gt-norm  ...     absent  norm  diaporthe-stem-canker\n",
              "4  october      normal  gt-norm  ...     absent  norm  diaporthe-stem-canker\n",
              "\n",
              "[5 rows x 36 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6mi8fw_uqGg",
        "outputId": "981cc7ec-fd98-481b-eb11-9a81f650bf63"
      },
      "source": [
        "dataset.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(683, 36)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzpkvD8xu6_I"
      },
      "source": [
        "X = dataset.iloc[:,:35]\n",
        "y = dataset.iloc[:,35]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMNXYbOQvOnu"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EG0tWkJOwB_4",
        "outputId": "39c6de5a-cc46-43e0-cfb5-e904d6c0ec31"
      },
      "source": [
        "for i in range(35):\n",
        "  X.iloc[:,i] = LabelEncoder().fit_transform(X.iloc[:,i])\n",
        "\n",
        "y = LabelEncoder().fit_transform(y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py:1781: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self.obj[item_labels[indexer[info_axis]]] = value\n",
            "/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py:1743: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  isetter(ilocs[0], value)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TSb0HZmxQAL"
      },
      "source": [
        "# Separando Treino e teste"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODrgZViGxKBM"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9vHrwGGyKSs"
      },
      "source": [
        "classifiers = [\n",
        "               RandomForestClassifier(100),\n",
        "               GaussianNB(),\n",
        "               DecisionTreeClassifier(),\n",
        "               KNeighborsClassifier(3)\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjM4aNI7xhWm"
      },
      "source": [
        "X_array = X.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFwvArfWx7MV"
      },
      "source": [
        "X_train_list, X_test_list, y_train_list, y_test_list = [], [], [], []\n",
        "\n",
        "rep = 20\n",
        "for i in range(rep):\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X_array, y, random_state = i)\n",
        "\n",
        "  X_train_list.append(X_train)\n",
        "  X_test_list.append(X_test)\n",
        "  y_train_list.append(y_train)\n",
        "  y_test_list.append(y_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVwPD9wwzkt1",
        "outputId": "c747a062-ba3a-4366-f300-1fe2088710cb"
      },
      "source": [
        "accs_mean = []\n",
        "for clf in classifiers:\n",
        "  acc = []\n",
        "  for i in range(rep):\n",
        "    clf.fit(X_train_list[i], y_train_list[i])\n",
        "    p = clf.predict(X_test_list[i])\n",
        "    acc.append(accuracy_score(p, y_test_list[i]))\n",
        "  accs_mean.append(np.mean(acc))\n",
        "  \n",
        "accs_mean"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9336257309941519,\n",
              " 0.8461988304093566,\n",
              " 0.9184210526315788,\n",
              " 0.8002923976608185]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIVDAq5E3jFm"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIrz-CDrPHuw"
      },
      "source": [
        "# Rede Neural"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LT2EoQqPSiq"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.utils import np_utils"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFFNyeY8Q-xg"
      },
      "source": [
        "y = np_utils.to_categorical(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8UG66jn3hwJ"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_array, y, random_state = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWO8VBo3PQVt"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(units= 36, input_dim= 35))\n",
        "\n",
        "model.add(Dense(units= 35))\n",
        "\n",
        "model.add(Dense(units= 19, activation= 'sigmoid'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pcwaqzfcRRCl",
        "outputId": "a8675c6a-bc8c-4623-b450-78a3a459b90b"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_3 (Dense)              (None, 36)                1296      \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 35)                1295      \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 19)                684       \n",
            "=================================================================\n",
            "Total params: 3,275\n",
            "Trainable params: 3,275\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOSmzjToRZAq"
      },
      "source": [
        "model.compile(optimizer= 'adam', loss= 'categorical_crossentropy', metrics= ['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQuqxDCkRmZ-",
        "outputId": "43045b2b-5f96-46a0-a83a-5b2cc062c9de"
      },
      "source": [
        "model.fit(X_train, y_train, epochs= 300,\n",
        "          validation_data= (X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "16/16 [==============================] - 1s 22ms/step - loss: 6.1121 - accuracy: 0.0294 - val_loss: 3.5791 - val_accuracy: 0.0643\n",
            "Epoch 2/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 3.0830 - accuracy: 0.1222 - val_loss: 2.7287 - val_accuracy: 0.2047\n",
            "Epoch 3/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.3735 - accuracy: 0.3299 - val_loss: 2.3942 - val_accuracy: 0.3626\n",
            "Epoch 4/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.0372 - accuracy: 0.4351 - val_loss: 2.1597 - val_accuracy: 0.3860\n",
            "Epoch 5/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1.8272 - accuracy: 0.4838 - val_loss: 1.9892 - val_accuracy: 0.4444\n",
            "Epoch 6/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1.5885 - accuracy: 0.5534 - val_loss: 1.8439 - val_accuracy: 0.4678\n",
            "Epoch 7/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1.5263 - accuracy: 0.5744 - val_loss: 1.7188 - val_accuracy: 0.5322\n",
            "Epoch 8/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1.4134 - accuracy: 0.5950 - val_loss: 1.6036 - val_accuracy: 0.5497\n",
            "Epoch 9/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1.2932 - accuracy: 0.6516 - val_loss: 1.4964 - val_accuracy: 0.5906\n",
            "Epoch 10/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1.1794 - accuracy: 0.6747 - val_loss: 1.4102 - val_accuracy: 0.6023\n",
            "Epoch 11/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1.1164 - accuracy: 0.6846 - val_loss: 1.3065 - val_accuracy: 0.6140\n",
            "Epoch 12/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.9755 - accuracy: 0.7474 - val_loss: 1.2372 - val_accuracy: 0.5965\n",
            "Epoch 13/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1.0026 - accuracy: 0.7189 - val_loss: 1.1770 - val_accuracy: 0.6374\n",
            "Epoch 14/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.9216 - accuracy: 0.7173 - val_loss: 1.0993 - val_accuracy: 0.6374\n",
            "Epoch 15/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.8599 - accuracy: 0.7511 - val_loss: 1.0444 - val_accuracy: 0.6550\n",
            "Epoch 16/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.7997 - accuracy: 0.7570 - val_loss: 1.0179 - val_accuracy: 0.6842\n",
            "Epoch 17/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.7641 - accuracy: 0.7867 - val_loss: 0.9479 - val_accuracy: 0.7076\n",
            "Epoch 18/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.6969 - accuracy: 0.8047 - val_loss: 0.9094 - val_accuracy: 0.7018\n",
            "Epoch 19/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.6586 - accuracy: 0.8275 - val_loss: 0.8770 - val_accuracy: 0.7310\n",
            "Epoch 20/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.6864 - accuracy: 0.8009 - val_loss: 0.8243 - val_accuracy: 0.7544\n",
            "Epoch 21/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.6510 - accuracy: 0.7857 - val_loss: 0.8071 - val_accuracy: 0.7485\n",
            "Epoch 22/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.6647 - accuracy: 0.7943 - val_loss: 0.7644 - val_accuracy: 0.7602\n",
            "Epoch 23/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.6139 - accuracy: 0.8136 - val_loss: 0.7429 - val_accuracy: 0.7602\n",
            "Epoch 24/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.5409 - accuracy: 0.8398 - val_loss: 0.7101 - val_accuracy: 0.7836\n",
            "Epoch 25/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.5571 - accuracy: 0.8183 - val_loss: 0.7026 - val_accuracy: 0.7953\n",
            "Epoch 26/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4800 - accuracy: 0.8684 - val_loss: 0.6738 - val_accuracy: 0.8012\n",
            "Epoch 27/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4541 - accuracy: 0.8685 - val_loss: 0.6538 - val_accuracy: 0.8129\n",
            "Epoch 28/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.5210 - accuracy: 0.8420 - val_loss: 0.6439 - val_accuracy: 0.8070\n",
            "Epoch 29/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.5130 - accuracy: 0.8467 - val_loss: 0.6223 - val_accuracy: 0.8187\n",
            "Epoch 30/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4673 - accuracy: 0.8512 - val_loss: 0.6005 - val_accuracy: 0.8246\n",
            "Epoch 31/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4641 - accuracy: 0.8550 - val_loss: 0.6135 - val_accuracy: 0.8012\n",
            "Epoch 32/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4231 - accuracy: 0.8585 - val_loss: 0.5753 - val_accuracy: 0.8246\n",
            "Epoch 33/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.4178 - accuracy: 0.8706 - val_loss: 0.6137 - val_accuracy: 0.7719\n",
            "Epoch 34/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4261 - accuracy: 0.8707 - val_loss: 0.5504 - val_accuracy: 0.8187\n",
            "Epoch 35/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4284 - accuracy: 0.8646 - val_loss: 0.5375 - val_accuracy: 0.8480\n",
            "Epoch 36/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4073 - accuracy: 0.8725 - val_loss: 0.5237 - val_accuracy: 0.8421\n",
            "Epoch 37/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4209 - accuracy: 0.8570 - val_loss: 0.5200 - val_accuracy: 0.8187\n",
            "Epoch 38/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3825 - accuracy: 0.8807 - val_loss: 0.5017 - val_accuracy: 0.8363\n",
            "Epoch 39/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3506 - accuracy: 0.9062 - val_loss: 0.4872 - val_accuracy: 0.8538\n",
            "Epoch 40/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3585 - accuracy: 0.8849 - val_loss: 0.4999 - val_accuracy: 0.8304\n",
            "Epoch 41/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3480 - accuracy: 0.8829 - val_loss: 0.4771 - val_accuracy: 0.8421\n",
            "Epoch 42/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3318 - accuracy: 0.8868 - val_loss: 0.4591 - val_accuracy: 0.8596\n",
            "Epoch 43/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3131 - accuracy: 0.9043 - val_loss: 0.4558 - val_accuracy: 0.8713\n",
            "Epoch 44/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3241 - accuracy: 0.8927 - val_loss: 0.4621 - val_accuracy: 0.8596\n",
            "Epoch 45/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3856 - accuracy: 0.8804 - val_loss: 0.4533 - val_accuracy: 0.8480\n",
            "Epoch 46/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4020 - accuracy: 0.8621 - val_loss: 0.4378 - val_accuracy: 0.8596\n",
            "Epoch 47/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3061 - accuracy: 0.9193 - val_loss: 0.4335 - val_accuracy: 0.8596\n",
            "Epoch 48/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3121 - accuracy: 0.9038 - val_loss: 0.4449 - val_accuracy: 0.8480\n",
            "Epoch 49/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3418 - accuracy: 0.8732 - val_loss: 0.4204 - val_accuracy: 0.8947\n",
            "Epoch 50/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3573 - accuracy: 0.8696 - val_loss: 0.4435 - val_accuracy: 0.8713\n",
            "Epoch 51/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3522 - accuracy: 0.8906 - val_loss: 0.4027 - val_accuracy: 0.8947\n",
            "Epoch 52/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2869 - accuracy: 0.9113 - val_loss: 0.4080 - val_accuracy: 0.8947\n",
            "Epoch 53/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2835 - accuracy: 0.9047 - val_loss: 0.3984 - val_accuracy: 0.9064\n",
            "Epoch 54/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2820 - accuracy: 0.9018 - val_loss: 0.3934 - val_accuracy: 0.8713\n",
            "Epoch 55/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3023 - accuracy: 0.8971 - val_loss: 0.3860 - val_accuracy: 0.9123\n",
            "Epoch 56/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2682 - accuracy: 0.9137 - val_loss: 0.3810 - val_accuracy: 0.9006\n",
            "Epoch 57/300\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.2888 - accuracy: 0.9026 - val_loss: 0.3906 - val_accuracy: 0.8713\n",
            "Epoch 58/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3025 - accuracy: 0.8849 - val_loss: 0.3816 - val_accuracy: 0.8713\n",
            "Epoch 59/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.2740 - accuracy: 0.9007 - val_loss: 0.3686 - val_accuracy: 0.9123\n",
            "Epoch 60/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3224 - accuracy: 0.8839 - val_loss: 0.3646 - val_accuracy: 0.8889\n",
            "Epoch 61/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.2582 - accuracy: 0.9198 - val_loss: 0.3912 - val_accuracy: 0.8713\n",
            "Epoch 62/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2495 - accuracy: 0.9192 - val_loss: 0.3572 - val_accuracy: 0.8889\n",
            "Epoch 63/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2391 - accuracy: 0.9220 - val_loss: 0.3568 - val_accuracy: 0.9123\n",
            "Epoch 64/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2435 - accuracy: 0.9326 - val_loss: 0.3659 - val_accuracy: 0.8830\n",
            "Epoch 65/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2760 - accuracy: 0.8968 - val_loss: 0.3512 - val_accuracy: 0.9123\n",
            "Epoch 66/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2561 - accuracy: 0.8996 - val_loss: 0.3481 - val_accuracy: 0.9181\n",
            "Epoch 67/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2961 - accuracy: 0.8965 - val_loss: 0.3797 - val_accuracy: 0.8655\n",
            "Epoch 68/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2443 - accuracy: 0.9195 - val_loss: 0.3443 - val_accuracy: 0.8889\n",
            "Epoch 69/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3115 - accuracy: 0.8975 - val_loss: 0.3439 - val_accuracy: 0.9123\n",
            "Epoch 70/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2494 - accuracy: 0.9148 - val_loss: 0.3422 - val_accuracy: 0.9006\n",
            "Epoch 71/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2677 - accuracy: 0.9038 - val_loss: 0.3442 - val_accuracy: 0.8947\n",
            "Epoch 72/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2895 - accuracy: 0.8960 - val_loss: 0.3406 - val_accuracy: 0.9006\n",
            "Epoch 73/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2372 - accuracy: 0.9129 - val_loss: 0.3352 - val_accuracy: 0.9064\n",
            "Epoch 74/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.2320 - accuracy: 0.9225 - val_loss: 0.3316 - val_accuracy: 0.8947\n",
            "Epoch 75/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2636 - accuracy: 0.8915 - val_loss: 0.3392 - val_accuracy: 0.8830\n",
            "Epoch 76/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2249 - accuracy: 0.9283 - val_loss: 0.3341 - val_accuracy: 0.8830\n",
            "Epoch 77/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2098 - accuracy: 0.9299 - val_loss: 0.3213 - val_accuracy: 0.8947\n",
            "Epoch 78/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2270 - accuracy: 0.9241 - val_loss: 0.3273 - val_accuracy: 0.8889\n",
            "Epoch 79/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2493 - accuracy: 0.8960 - val_loss: 0.3219 - val_accuracy: 0.8889\n",
            "Epoch 80/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2161 - accuracy: 0.9269 - val_loss: 0.3300 - val_accuracy: 0.9006\n",
            "Epoch 81/300\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.2393 - accuracy: 0.9028 - val_loss: 0.3236 - val_accuracy: 0.8947\n",
            "Epoch 82/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2489 - accuracy: 0.9277 - val_loss: 0.3247 - val_accuracy: 0.8889\n",
            "Epoch 83/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2093 - accuracy: 0.9176 - val_loss: 0.3174 - val_accuracy: 0.8830\n",
            "Epoch 84/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2339 - accuracy: 0.9187 - val_loss: 0.3597 - val_accuracy: 0.8655\n",
            "Epoch 85/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2917 - accuracy: 0.8888 - val_loss: 0.3132 - val_accuracy: 0.9064\n",
            "Epoch 86/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1973 - accuracy: 0.9226 - val_loss: 0.3534 - val_accuracy: 0.8655\n",
            "Epoch 87/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2599 - accuracy: 0.8888 - val_loss: 0.3077 - val_accuracy: 0.9123\n",
            "Epoch 88/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2328 - accuracy: 0.9127 - val_loss: 0.3082 - val_accuracy: 0.8947\n",
            "Epoch 89/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2264 - accuracy: 0.9126 - val_loss: 0.3182 - val_accuracy: 0.9006\n",
            "Epoch 90/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1985 - accuracy: 0.9374 - val_loss: 0.3113 - val_accuracy: 0.9006\n",
            "Epoch 91/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2356 - accuracy: 0.9064 - val_loss: 0.3031 - val_accuracy: 0.9006\n",
            "Epoch 92/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2535 - accuracy: 0.8915 - val_loss: 0.3065 - val_accuracy: 0.9123\n",
            "Epoch 93/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2061 - accuracy: 0.9338 - val_loss: 0.3086 - val_accuracy: 0.8830\n",
            "Epoch 94/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2182 - accuracy: 0.9234 - val_loss: 0.3162 - val_accuracy: 0.8889\n",
            "Epoch 95/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2329 - accuracy: 0.9136 - val_loss: 0.3023 - val_accuracy: 0.8889\n",
            "Epoch 96/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2554 - accuracy: 0.9001 - val_loss: 0.3013 - val_accuracy: 0.9006\n",
            "Epoch 97/300\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1922 - accuracy: 0.9226 - val_loss: 0.3064 - val_accuracy: 0.8889\n",
            "Epoch 98/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2123 - accuracy: 0.9210 - val_loss: 0.3119 - val_accuracy: 0.8830\n",
            "Epoch 99/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2212 - accuracy: 0.9143 - val_loss: 0.3037 - val_accuracy: 0.8655\n",
            "Epoch 100/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2295 - accuracy: 0.9002 - val_loss: 0.2947 - val_accuracy: 0.9064\n",
            "Epoch 101/300\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.2148 - accuracy: 0.9154 - val_loss: 0.3037 - val_accuracy: 0.9006\n",
            "Epoch 102/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2031 - accuracy: 0.9211 - val_loss: 0.2999 - val_accuracy: 0.8947\n",
            "Epoch 103/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2131 - accuracy: 0.9139 - val_loss: 0.2966 - val_accuracy: 0.9006\n",
            "Epoch 104/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2207 - accuracy: 0.9155 - val_loss: 0.2966 - val_accuracy: 0.9064\n",
            "Epoch 105/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2505 - accuracy: 0.8889 - val_loss: 0.3440 - val_accuracy: 0.8596\n",
            "Epoch 106/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2397 - accuracy: 0.9222 - val_loss: 0.2892 - val_accuracy: 0.9064\n",
            "Epoch 107/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2057 - accuracy: 0.9141 - val_loss: 0.3064 - val_accuracy: 0.8772\n",
            "Epoch 108/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2119 - accuracy: 0.9207 - val_loss: 0.2907 - val_accuracy: 0.9006\n",
            "Epoch 109/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2102 - accuracy: 0.9145 - val_loss: 0.2890 - val_accuracy: 0.9064\n",
            "Epoch 110/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1804 - accuracy: 0.9360 - val_loss: 0.2984 - val_accuracy: 0.8947\n",
            "Epoch 111/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2252 - accuracy: 0.9007 - val_loss: 0.3007 - val_accuracy: 0.8772\n",
            "Epoch 112/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2105 - accuracy: 0.9243 - val_loss: 0.2912 - val_accuracy: 0.9123\n",
            "Epoch 113/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1907 - accuracy: 0.9319 - val_loss: 0.3011 - val_accuracy: 0.8772\n",
            "Epoch 114/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2434 - accuracy: 0.9088 - val_loss: 0.2889 - val_accuracy: 0.9006\n",
            "Epoch 115/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2088 - accuracy: 0.9097 - val_loss: 0.2891 - val_accuracy: 0.8830\n",
            "Epoch 116/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2005 - accuracy: 0.9355 - val_loss: 0.2941 - val_accuracy: 0.8947\n",
            "Epoch 117/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2106 - accuracy: 0.9081 - val_loss: 0.2880 - val_accuracy: 0.8947\n",
            "Epoch 118/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2014 - accuracy: 0.9182 - val_loss: 0.2874 - val_accuracy: 0.8947\n",
            "Epoch 119/300\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.2087 - accuracy: 0.9206 - val_loss: 0.3223 - val_accuracy: 0.8772\n",
            "Epoch 120/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2349 - accuracy: 0.9117 - val_loss: 0.2905 - val_accuracy: 0.8830\n",
            "Epoch 121/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2036 - accuracy: 0.9214 - val_loss: 0.3190 - val_accuracy: 0.8772\n",
            "Epoch 122/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2233 - accuracy: 0.9153 - val_loss: 0.2965 - val_accuracy: 0.8889\n",
            "Epoch 123/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2002 - accuracy: 0.9282 - val_loss: 0.2909 - val_accuracy: 0.8947\n",
            "Epoch 124/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1551 - accuracy: 0.9367 - val_loss: 0.2920 - val_accuracy: 0.8830\n",
            "Epoch 125/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1888 - accuracy: 0.9212 - val_loss: 0.2809 - val_accuracy: 0.9006\n",
            "Epoch 126/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1861 - accuracy: 0.9261 - val_loss: 0.2880 - val_accuracy: 0.8889\n",
            "Epoch 127/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.1971 - accuracy: 0.9229 - val_loss: 0.2885 - val_accuracy: 0.8889\n",
            "Epoch 128/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1783 - accuracy: 0.9305 - val_loss: 0.2826 - val_accuracy: 0.8889\n",
            "Epoch 129/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2484 - accuracy: 0.8991 - val_loss: 0.2813 - val_accuracy: 0.9006\n",
            "Epoch 130/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2027 - accuracy: 0.9182 - val_loss: 0.2897 - val_accuracy: 0.8889\n",
            "Epoch 131/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2144 - accuracy: 0.9117 - val_loss: 0.2800 - val_accuracy: 0.9006\n",
            "Epoch 132/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2347 - accuracy: 0.9166 - val_loss: 0.2871 - val_accuracy: 0.9006\n",
            "Epoch 133/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1854 - accuracy: 0.9181 - val_loss: 0.2829 - val_accuracy: 0.9006\n",
            "Epoch 134/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1886 - accuracy: 0.9137 - val_loss: 0.2956 - val_accuracy: 0.8830\n",
            "Epoch 135/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2230 - accuracy: 0.9130 - val_loss: 0.3089 - val_accuracy: 0.8830\n",
            "Epoch 136/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1865 - accuracy: 0.9230 - val_loss: 0.2835 - val_accuracy: 0.8889\n",
            "Epoch 137/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1953 - accuracy: 0.9139 - val_loss: 0.2865 - val_accuracy: 0.9006\n",
            "Epoch 138/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.1695 - accuracy: 0.9342 - val_loss: 0.2941 - val_accuracy: 0.8889\n",
            "Epoch 139/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.2184 - accuracy: 0.9072 - val_loss: 0.3066 - val_accuracy: 0.8655\n",
            "Epoch 140/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2075 - accuracy: 0.9253 - val_loss: 0.2792 - val_accuracy: 0.8947\n",
            "Epoch 141/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2066 - accuracy: 0.9230 - val_loss: 0.2826 - val_accuracy: 0.8889\n",
            "Epoch 142/300\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.2063 - accuracy: 0.9193 - val_loss: 0.2862 - val_accuracy: 0.8772\n",
            "Epoch 143/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2261 - accuracy: 0.9098 - val_loss: 0.3067 - val_accuracy: 0.8655\n",
            "Epoch 144/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2312 - accuracy: 0.9210 - val_loss: 0.2796 - val_accuracy: 0.8947\n",
            "Epoch 145/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1927 - accuracy: 0.9194 - val_loss: 0.2847 - val_accuracy: 0.8830\n",
            "Epoch 146/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2062 - accuracy: 0.9197 - val_loss: 0.3142 - val_accuracy: 0.8713\n",
            "Epoch 147/300\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.2473 - accuracy: 0.9093 - val_loss: 0.2832 - val_accuracy: 0.9064\n",
            "Epoch 148/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2169 - accuracy: 0.9179 - val_loss: 0.2960 - val_accuracy: 0.8772\n",
            "Epoch 149/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1618 - accuracy: 0.9404 - val_loss: 0.2784 - val_accuracy: 0.8772\n",
            "Epoch 150/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2240 - accuracy: 0.9192 - val_loss: 0.3084 - val_accuracy: 0.8713\n",
            "Epoch 151/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1787 - accuracy: 0.9177 - val_loss: 0.3084 - val_accuracy: 0.8889\n",
            "Epoch 152/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2272 - accuracy: 0.9072 - val_loss: 0.2798 - val_accuracy: 0.8889\n",
            "Epoch 153/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1745 - accuracy: 0.9334 - val_loss: 0.2773 - val_accuracy: 0.8947\n",
            "Epoch 154/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1777 - accuracy: 0.9350 - val_loss: 0.2777 - val_accuracy: 0.8947\n",
            "Epoch 155/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2052 - accuracy: 0.9216 - val_loss: 0.2799 - val_accuracy: 0.9006\n",
            "Epoch 156/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1816 - accuracy: 0.9172 - val_loss: 0.2804 - val_accuracy: 0.8772\n",
            "Epoch 157/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1703 - accuracy: 0.9341 - val_loss: 0.2929 - val_accuracy: 0.8889\n",
            "Epoch 158/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1820 - accuracy: 0.9283 - val_loss: 0.2739 - val_accuracy: 0.8947\n",
            "Epoch 159/300\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.2055 - accuracy: 0.9249 - val_loss: 0.2800 - val_accuracy: 0.8889\n",
            "Epoch 160/300\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.2037 - accuracy: 0.9168 - val_loss: 0.2754 - val_accuracy: 0.9064\n",
            "Epoch 161/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1658 - accuracy: 0.9249 - val_loss: 0.2739 - val_accuracy: 0.8947\n",
            "Epoch 162/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2272 - accuracy: 0.8950 - val_loss: 0.2912 - val_accuracy: 0.8830\n",
            "Epoch 163/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2231 - accuracy: 0.9219 - val_loss: 0.2744 - val_accuracy: 0.9064\n",
            "Epoch 164/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1892 - accuracy: 0.9218 - val_loss: 0.2804 - val_accuracy: 0.8713\n",
            "Epoch 165/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1887 - accuracy: 0.9284 - val_loss: 0.2750 - val_accuracy: 0.9123\n",
            "Epoch 166/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1841 - accuracy: 0.9245 - val_loss: 0.2762 - val_accuracy: 0.8889\n",
            "Epoch 167/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2082 - accuracy: 0.9113 - val_loss: 0.2813 - val_accuracy: 0.8947\n",
            "Epoch 168/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1615 - accuracy: 0.9272 - val_loss: 0.2795 - val_accuracy: 0.8889\n",
            "Epoch 169/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2005 - accuracy: 0.9172 - val_loss: 0.2931 - val_accuracy: 0.8889\n",
            "Epoch 170/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2326 - accuracy: 0.8992 - val_loss: 0.2775 - val_accuracy: 0.8830\n",
            "Epoch 171/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1866 - accuracy: 0.9310 - val_loss: 0.2764 - val_accuracy: 0.8947\n",
            "Epoch 172/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1782 - accuracy: 0.9242 - val_loss: 0.2785 - val_accuracy: 0.8830\n",
            "Epoch 173/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1772 - accuracy: 0.9421 - val_loss: 0.2861 - val_accuracy: 0.9006\n",
            "Epoch 174/300\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1725 - accuracy: 0.9256 - val_loss: 0.2795 - val_accuracy: 0.8830\n",
            "Epoch 175/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2290 - accuracy: 0.9124 - val_loss: 0.2715 - val_accuracy: 0.8947\n",
            "Epoch 176/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1738 - accuracy: 0.9381 - val_loss: 0.2851 - val_accuracy: 0.8830\n",
            "Epoch 177/300\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1575 - accuracy: 0.9391 - val_loss: 0.2806 - val_accuracy: 0.8830\n",
            "Epoch 178/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1671 - accuracy: 0.9316 - val_loss: 0.2827 - val_accuracy: 0.8889\n",
            "Epoch 179/300\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1970 - accuracy: 0.9317 - val_loss: 0.2743 - val_accuracy: 0.8830\n",
            "Epoch 180/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2183 - accuracy: 0.9014 - val_loss: 0.2890 - val_accuracy: 0.8772\n",
            "Epoch 181/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1667 - accuracy: 0.9350 - val_loss: 0.2832 - val_accuracy: 0.9006\n",
            "Epoch 182/300\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1985 - accuracy: 0.9248 - val_loss: 0.2792 - val_accuracy: 0.8889\n",
            "Epoch 183/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1915 - accuracy: 0.9225 - val_loss: 0.2777 - val_accuracy: 0.9064\n",
            "Epoch 184/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1424 - accuracy: 0.9405 - val_loss: 0.2770 - val_accuracy: 0.8830\n",
            "Epoch 185/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1604 - accuracy: 0.9384 - val_loss: 0.2920 - val_accuracy: 0.8889\n",
            "Epoch 186/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1827 - accuracy: 0.9179 - val_loss: 0.2714 - val_accuracy: 0.8947\n",
            "Epoch 187/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1875 - accuracy: 0.9237 - val_loss: 0.2878 - val_accuracy: 0.8947\n",
            "Epoch 188/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1685 - accuracy: 0.9310 - val_loss: 0.2724 - val_accuracy: 0.8889\n",
            "Epoch 189/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1965 - accuracy: 0.9231 - val_loss: 0.2811 - val_accuracy: 0.8947\n",
            "Epoch 190/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1841 - accuracy: 0.9173 - val_loss: 0.2762 - val_accuracy: 0.8772\n",
            "Epoch 191/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2139 - accuracy: 0.9035 - val_loss: 0.2773 - val_accuracy: 0.8713\n",
            "Epoch 192/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2003 - accuracy: 0.9186 - val_loss: 0.2709 - val_accuracy: 0.9006\n",
            "Epoch 193/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1800 - accuracy: 0.9219 - val_loss: 0.2878 - val_accuracy: 0.8889\n",
            "Epoch 194/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2052 - accuracy: 0.9149 - val_loss: 0.2759 - val_accuracy: 0.9123\n",
            "Epoch 195/300\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1921 - accuracy: 0.9234 - val_loss: 0.2733 - val_accuracy: 0.8772\n",
            "Epoch 196/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1713 - accuracy: 0.9272 - val_loss: 0.2979 - val_accuracy: 0.8830\n",
            "Epoch 197/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1959 - accuracy: 0.9223 - val_loss: 0.2779 - val_accuracy: 0.8830\n",
            "Epoch 198/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.1746 - accuracy: 0.9332 - val_loss: 0.2787 - val_accuracy: 0.8889\n",
            "Epoch 199/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1763 - accuracy: 0.9302 - val_loss: 0.2730 - val_accuracy: 0.9006\n",
            "Epoch 200/300\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.2206 - accuracy: 0.9069 - val_loss: 0.2753 - val_accuracy: 0.8830\n",
            "Epoch 201/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1767 - accuracy: 0.9255 - val_loss: 0.2827 - val_accuracy: 0.9006\n",
            "Epoch 202/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1874 - accuracy: 0.9219 - val_loss: 0.2735 - val_accuracy: 0.8889\n",
            "Epoch 203/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1582 - accuracy: 0.9255 - val_loss: 0.2715 - val_accuracy: 0.8889\n",
            "Epoch 204/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2218 - accuracy: 0.9191 - val_loss: 0.2882 - val_accuracy: 0.8830\n",
            "Epoch 205/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1714 - accuracy: 0.9338 - val_loss: 0.2990 - val_accuracy: 0.8713\n",
            "Epoch 206/300\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.2150 - accuracy: 0.8979 - val_loss: 0.2932 - val_accuracy: 0.8889\n",
            "Epoch 207/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1857 - accuracy: 0.9136 - val_loss: 0.2765 - val_accuracy: 0.8713\n",
            "Epoch 208/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1703 - accuracy: 0.9233 - val_loss: 0.2760 - val_accuracy: 0.8772\n",
            "Epoch 209/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2157 - accuracy: 0.9283 - val_loss: 0.2836 - val_accuracy: 0.9006\n",
            "Epoch 210/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1505 - accuracy: 0.9454 - val_loss: 0.2716 - val_accuracy: 0.8830\n",
            "Epoch 211/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1787 - accuracy: 0.9248 - val_loss: 0.2692 - val_accuracy: 0.8889\n",
            "Epoch 212/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1781 - accuracy: 0.9227 - val_loss: 0.2820 - val_accuracy: 0.9064\n",
            "Epoch 213/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1972 - accuracy: 0.9185 - val_loss: 0.2692 - val_accuracy: 0.8889\n",
            "Epoch 214/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1728 - accuracy: 0.9306 - val_loss: 0.2821 - val_accuracy: 0.8830\n",
            "Epoch 215/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1552 - accuracy: 0.9340 - val_loss: 0.2705 - val_accuracy: 0.9064\n",
            "Epoch 216/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1632 - accuracy: 0.9320 - val_loss: 0.2740 - val_accuracy: 0.8889\n",
            "Epoch 217/300\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1476 - accuracy: 0.9346 - val_loss: 0.2735 - val_accuracy: 0.9064\n",
            "Epoch 218/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1633 - accuracy: 0.9299 - val_loss: 0.2698 - val_accuracy: 0.9006\n",
            "Epoch 219/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1957 - accuracy: 0.9057 - val_loss: 0.2766 - val_accuracy: 0.8947\n",
            "Epoch 220/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1841 - accuracy: 0.9112 - val_loss: 0.3152 - val_accuracy: 0.8655\n",
            "Epoch 221/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1584 - accuracy: 0.9300 - val_loss: 0.3001 - val_accuracy: 0.8596\n",
            "Epoch 222/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2002 - accuracy: 0.8983 - val_loss: 0.2937 - val_accuracy: 0.9006\n",
            "Epoch 223/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1745 - accuracy: 0.9202 - val_loss: 0.2731 - val_accuracy: 0.8713\n",
            "Epoch 224/300\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1867 - accuracy: 0.9144 - val_loss: 0.2824 - val_accuracy: 0.8947\n",
            "Epoch 225/300\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1440 - accuracy: 0.9463 - val_loss: 0.2772 - val_accuracy: 0.9006\n",
            "Epoch 226/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1802 - accuracy: 0.9191 - val_loss: 0.2818 - val_accuracy: 0.8830\n",
            "Epoch 227/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1636 - accuracy: 0.9346 - val_loss: 0.2755 - val_accuracy: 0.8889\n",
            "Epoch 228/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1940 - accuracy: 0.9259 - val_loss: 0.2746 - val_accuracy: 0.9006\n",
            "Epoch 229/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1804 - accuracy: 0.9160 - val_loss: 0.2701 - val_accuracy: 0.9006\n",
            "Epoch 230/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1744 - accuracy: 0.9188 - val_loss: 0.2832 - val_accuracy: 0.8830\n",
            "Epoch 231/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1597 - accuracy: 0.9384 - val_loss: 0.2932 - val_accuracy: 0.8947\n",
            "Epoch 232/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1780 - accuracy: 0.9075 - val_loss: 0.2703 - val_accuracy: 0.8889\n",
            "Epoch 233/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1797 - accuracy: 0.9211 - val_loss: 0.2726 - val_accuracy: 0.9064\n",
            "Epoch 234/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1574 - accuracy: 0.9269 - val_loss: 0.2735 - val_accuracy: 0.8947\n",
            "Epoch 235/300\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1826 - accuracy: 0.9236 - val_loss: 0.2817 - val_accuracy: 0.8772\n",
            "Epoch 236/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2142 - accuracy: 0.9070 - val_loss: 0.2769 - val_accuracy: 0.8830\n",
            "Epoch 237/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1880 - accuracy: 0.9133 - val_loss: 0.2740 - val_accuracy: 0.9123\n",
            "Epoch 238/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1554 - accuracy: 0.9406 - val_loss: 0.2739 - val_accuracy: 0.9064\n",
            "Epoch 239/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1957 - accuracy: 0.9153 - val_loss: 0.2751 - val_accuracy: 0.9064\n",
            "Epoch 240/300\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1499 - accuracy: 0.9302 - val_loss: 0.2749 - val_accuracy: 0.8889\n",
            "Epoch 241/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1976 - accuracy: 0.9142 - val_loss: 0.2825 - val_accuracy: 0.8772\n",
            "Epoch 242/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1811 - accuracy: 0.9120 - val_loss: 0.2702 - val_accuracy: 0.9006\n",
            "Epoch 243/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1687 - accuracy: 0.9348 - val_loss: 0.2857 - val_accuracy: 0.8830\n",
            "Epoch 244/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1482 - accuracy: 0.9321 - val_loss: 0.2694 - val_accuracy: 0.8889\n",
            "Epoch 245/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1783 - accuracy: 0.9110 - val_loss: 0.2767 - val_accuracy: 0.8713\n",
            "Epoch 246/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2209 - accuracy: 0.9051 - val_loss: 0.2685 - val_accuracy: 0.9064\n",
            "Epoch 247/300\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.2112 - accuracy: 0.9113 - val_loss: 0.2894 - val_accuracy: 0.9064\n",
            "Epoch 248/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1763 - accuracy: 0.9287 - val_loss: 0.2776 - val_accuracy: 0.8830\n",
            "Epoch 249/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1696 - accuracy: 0.9369 - val_loss: 0.2700 - val_accuracy: 0.8772\n",
            "Epoch 250/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1827 - accuracy: 0.9290 - val_loss: 0.2769 - val_accuracy: 0.9006\n",
            "Epoch 251/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1648 - accuracy: 0.9362 - val_loss: 0.2710 - val_accuracy: 0.8889\n",
            "Epoch 252/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1594 - accuracy: 0.9385 - val_loss: 0.2733 - val_accuracy: 0.9064\n",
            "Epoch 253/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1378 - accuracy: 0.9426 - val_loss: 0.2703 - val_accuracy: 0.8889\n",
            "Epoch 254/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1867 - accuracy: 0.9253 - val_loss: 0.2728 - val_accuracy: 0.8830\n",
            "Epoch 255/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1940 - accuracy: 0.9176 - val_loss: 0.2849 - val_accuracy: 0.8889\n",
            "Epoch 256/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1795 - accuracy: 0.9148 - val_loss: 0.3015 - val_accuracy: 0.8889\n",
            "Epoch 257/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1676 - accuracy: 0.9309 - val_loss: 0.2726 - val_accuracy: 0.8830\n",
            "Epoch 258/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1574 - accuracy: 0.9412 - val_loss: 0.3101 - val_accuracy: 0.8889\n",
            "Epoch 259/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1952 - accuracy: 0.9172 - val_loss: 0.2755 - val_accuracy: 0.8830\n",
            "Epoch 260/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1513 - accuracy: 0.9333 - val_loss: 0.2807 - val_accuracy: 0.9064\n",
            "Epoch 261/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1628 - accuracy: 0.9168 - val_loss: 0.2769 - val_accuracy: 0.9064\n",
            "Epoch 262/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1730 - accuracy: 0.9369 - val_loss: 0.2868 - val_accuracy: 0.8947\n",
            "Epoch 263/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1699 - accuracy: 0.9346 - val_loss: 0.2758 - val_accuracy: 0.8830\n",
            "Epoch 264/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1585 - accuracy: 0.9301 - val_loss: 0.2889 - val_accuracy: 0.9064\n",
            "Epoch 265/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1754 - accuracy: 0.9366 - val_loss: 0.2881 - val_accuracy: 0.8947\n",
            "Epoch 266/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1796 - accuracy: 0.9316 - val_loss: 0.2760 - val_accuracy: 0.9006\n",
            "Epoch 267/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1795 - accuracy: 0.9233 - val_loss: 0.3042 - val_accuracy: 0.8596\n",
            "Epoch 268/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2069 - accuracy: 0.9145 - val_loss: 0.2911 - val_accuracy: 0.9006\n",
            "Epoch 269/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1595 - accuracy: 0.9376 - val_loss: 0.2773 - val_accuracy: 0.8830\n",
            "Epoch 270/300\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.2091 - accuracy: 0.9109 - val_loss: 0.2725 - val_accuracy: 0.9006\n",
            "Epoch 271/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1796 - accuracy: 0.9335 - val_loss: 0.2767 - val_accuracy: 0.8830\n",
            "Epoch 272/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1753 - accuracy: 0.9136 - val_loss: 0.2705 - val_accuracy: 0.9006\n",
            "Epoch 273/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1791 - accuracy: 0.9223 - val_loss: 0.2809 - val_accuracy: 0.8889\n",
            "Epoch 274/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1748 - accuracy: 0.9224 - val_loss: 0.2691 - val_accuracy: 0.9123\n",
            "Epoch 275/300\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1823 - accuracy: 0.9155 - val_loss: 0.2929 - val_accuracy: 0.8889\n",
            "Epoch 276/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1586 - accuracy: 0.9448 - val_loss: 0.2727 - val_accuracy: 0.9123\n",
            "Epoch 277/300\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1822 - accuracy: 0.9262 - val_loss: 0.2746 - val_accuracy: 0.8830\n",
            "Epoch 278/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1583 - accuracy: 0.9230 - val_loss: 0.2753 - val_accuracy: 0.9064\n",
            "Epoch 279/300\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1396 - accuracy: 0.9398 - val_loss: 0.2816 - val_accuracy: 0.8889\n",
            "Epoch 280/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1620 - accuracy: 0.9244 - val_loss: 0.2759 - val_accuracy: 0.9064\n",
            "Epoch 281/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1941 - accuracy: 0.9162 - val_loss: 0.2721 - val_accuracy: 0.8889\n",
            "Epoch 282/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1978 - accuracy: 0.9132 - val_loss: 0.2757 - val_accuracy: 0.9064\n",
            "Epoch 283/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2149 - accuracy: 0.9104 - val_loss: 0.2691 - val_accuracy: 0.8889\n",
            "Epoch 284/300\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1777 - accuracy: 0.9163 - val_loss: 0.2761 - val_accuracy: 0.9064\n",
            "Epoch 285/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1634 - accuracy: 0.9282 - val_loss: 0.2709 - val_accuracy: 0.8947\n",
            "Epoch 286/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1537 - accuracy: 0.9441 - val_loss: 0.2781 - val_accuracy: 0.9006\n",
            "Epoch 287/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1943 - accuracy: 0.9150 - val_loss: 0.2721 - val_accuracy: 0.8947\n",
            "Epoch 288/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2064 - accuracy: 0.9100 - val_loss: 0.2738 - val_accuracy: 0.8947\n",
            "Epoch 289/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1513 - accuracy: 0.9397 - val_loss: 0.2722 - val_accuracy: 0.9064\n",
            "Epoch 290/300\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1542 - accuracy: 0.9334 - val_loss: 0.2703 - val_accuracy: 0.8889\n",
            "Epoch 291/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1607 - accuracy: 0.9377 - val_loss: 0.2776 - val_accuracy: 0.8947\n",
            "Epoch 292/300\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.2042 - accuracy: 0.9061 - val_loss: 0.2887 - val_accuracy: 0.9064\n",
            "Epoch 293/300\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1623 - accuracy: 0.9318 - val_loss: 0.2953 - val_accuracy: 0.8772\n",
            "Epoch 294/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1956 - accuracy: 0.9126 - val_loss: 0.2931 - val_accuracy: 0.8947\n",
            "Epoch 295/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1737 - accuracy: 0.9255 - val_loss: 0.2746 - val_accuracy: 0.8889\n",
            "Epoch 296/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1813 - accuracy: 0.9353 - val_loss: 0.2823 - val_accuracy: 0.9006\n",
            "Epoch 297/300\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1615 - accuracy: 0.9141 - val_loss: 0.2711 - val_accuracy: 0.9123\n",
            "Epoch 298/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1447 - accuracy: 0.9393 - val_loss: 0.2992 - val_accuracy: 0.8889\n",
            "Epoch 299/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1766 - accuracy: 0.9249 - val_loss: 0.2878 - val_accuracy: 0.8713\n",
            "Epoch 300/300\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.2101 - accuracy: 0.9165 - val_loss: 0.2874 - val_accuracy: 0.8830\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fcf97acf210>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    }
  ]
}